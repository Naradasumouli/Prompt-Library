{
  "tcg": [
	  {
		"serial": "1",
		"author": "Moulivenkat Naradasu",
		"functionality": "ChargeSession",
		"prompt": "https://cognizantonline.sharepoint.com/:t:/r/sites/ABBTITAN-1000425402Cognizant/Shared%20Documents/General/POC/ChargeSession.prompt.md?csf=1&web=1&e=XbAWBx",
		"context": "Generate EV charger automation test cases by referencing existing \"ChargeSessions\" folder. Create realistic, workspace-compliant scenarios using only approved keywords. Organize cases from Basic to Intermediate (positive, negative, edge), ensuring no duplication and strict adherence to Robot Framework standards. Include proper setup/teardown and reflect real-world EV charging workflows.",
		"version": "1.0"
	  },
	  {
		"serial": "2",
		"author": "Moulivenkat Naradasu",
		"functionality": "Authorization",
		"prompt": "Vehicle/User Authorization                    \n\n\n\n\n\n# Role:\nYou are an EV charger automation testing expert with strong knowledge of functionality validation, scenario design, and real-world coverage.\n \n# Goal:\nTake a references of folder \"Authorization\", generate test cases by taking reference to test cases present in this folder. Prefer similar type of test cases. Generate a full set of test scenarios from Basic → Intermediate.\nScenarios must be realistic, workspace-compliant, and avoid duplication.\n \n# Instructions:\n \nGenerate Test Cases by Category\nBasic (Positive, Negative, Edge case): Standard workflows, happy paths, normal conditions.\nIntermediate (Positive, Negative, Edge case): Invalid inputs, error handling, network drops, recovery, user mistakes.\nValidate: Make sure scenarios match real-world use, build from simple to complex, and stay authentic.\n \n# Quality & Standards\n \nUse only existing workspace keywords, For references check with Keywords folder in workspace.\nFollow established naming, formatting, and documentation standards.\nEnsure no duplication of existing test cases.\nKeep scenarios progressive (Basic → Intermediate) and tied to real-world EV charging conditions.\n \n# Output Format (for each case):\n \nTitle (workspace naming style)\n \nObjective\n \nStep-by-step actions (workspace keywords only)\n \n# Expected Result:\n \nCategory: Basic (Positive, Negative, Edge case) / Intermediate (Positive, Negative, Edge case)\n\nWhen analyzing files in my workspace, verify that each test case includes appropriate preconditions and postconditions, following the approach used in Robot Framework.\n\nFor new files, always define Suite Setup and Suite Teardown, Include Test Setup and Test Teardown for test cases files.\n\nApply these checks consistently across both existing and newly created files to ensure proper initialization and cleanup.\n\n# Constraints:\n \nNo new keywords or functions\n \nNo duplicate cases\n \nStrict adherence to workspace standards\n \nEV charger–specific scenarios only",
		"context": "Generate EV charger automation test cases by referencing existing \"Authorization\" folder. Create realistic, workspace-compliant scenarios using only approved keywords. Organize cases from Basic to Intermediate (positive, negative, edge), ensuring no duplication and strict adherence to Robot Framework standards. Include proper setup/teardown and reflect real-world EV charging workflows.",
		"version": "1.0"
	  },
	  {
		"serial": "3",
		"author": "Moulivenkat Naradasu",
		"functionality": "OCPP Features",
		"prompt": "OCPP Features                                          \n\n\nYour mission is to produce deep, advanced, real-world test coverage for all OCPP (Open Charge Point Protocol) features.  \n\n# 1. Pre-Execution Analysis  \n1. Perform a deep review of:  \n   - Existing test suites, folders, resource files and keywords library  \n   - Scheduler and profile implementations (to spot overlaps)  \n   - OCPP API definitions and message flows  \n2. Identify gaps in coverage and document any duplicated logic.  \n3. Update your keyword map and test structure to reflect only *new* scenarios.  \n\n# 2. Target OCPP Features  \nCover at minimum:  \n- Remote Start Transaction  \n- Remote Stop Transaction  \n- Change Availability (Operative / Inoperative)  \n- Hard Reset and Soft Reset, including during an active session  \n- Boot Notification & Heartbeat (connectivity checks)  \n- Meter Values & Status Notification messages  \n- Authorization and Reservation flows  \n- following but not limited to your deep thinking\n  \n# 3. Test-Case Structure  \nFor each feature, deliver:  \n1. **Preconditions** – Charger state, network, user credentials, connector status  \n2. **Steps** – Sequence of API calls, connector actions, timing details  \n3. **Test Data** – RFID tags, timestamps, payload samples  \n4. **Expected Results** – Charger responses, OCPP message contents, state transitions  \n5. **Logs** – Meaningful entries (no empty lines), referencing keywords and checkpoints  \n6. **Cleanup** – Restore charger to default state  \n\n\n# 4. Scenario Types  \nGenerate for *each* feature:  \n- Positive scenarios (valid requests, normal timing, ideal network)  \n- Negative scenarios (invalid payloads, unauthorized RFID, lost heartbeat)  \n- Edge cases (simultaneous commands, mid-session resets, high-latency networks)  \n- following but not limited to your deep thinking\n\n# 5. Delivery Guidelines  \n- Use existing keyword naming conventions; add new ones only when necessary  \n- Avoid duplicating any pre-tested flows  \n- Organize results in Markdown with clear headings and bullet lists  \n- Ensure blank lines before and after each paragraph for readability  \n- Include a summary table of test cases by feature ",
		"context": "Design comprehensive, real-world test coverage for all major OCPP features. Analyze existing test assets to avoid duplication, then generate structured test cases (positive, negative, edge) for each feature using workspace-compliant keywords. Ensure each case includes preconditions, steps, test data, expected results, logs, and cleanup. Organize output in Markdown with a summary table.",
		"version": "1.0"
	  }
  ],
  "dbg": [
	  {
		"serial": "1",
		"author": "Moulivenkat Naradasu",
		"functionality": "Regression Report Analysis",
		"prompt": "## Task\nAnalyze a given **Robot Framework HTML report** and produce a clear, human‑readable summary of all test results, including **root cause analysis** for failures.\n\n\n## Steps to Follow\n1. **Open and review** the provided Robot Framework `C:\\Projects\\ABB\\Titan\\charger-test-framework-titan\\debug\\TestCaseLogs\\01\\report.html` file.  \n2. **Identify** how the following are stored in the report:  \n   - Test case names  \n   - Keywords used in each test  \n   - Status of each test (Passed, Failed, Skipped)  \n   - Messages, logs, and error details  \n3. **Extract** all test cases and list them with their status.  \n4. **For failed test cases**, also include:  \n   - **Failed keyword** (the keyword where the failure occurred)  \n   - **Failure reason/message** rewritten in **plain language** for non‑technical readers  \n   - **Detailed failure summary** (technical details from the report)  \n   - **Expected vs. actual values**  \n   - **Root cause analysis** at the *expert level*:  \n     - Explain what the test was expecting to happen  \n     - Explain what actually happened  \n     - Identify the most likely cause based only on visible evidence in the report (no assumptions beyond what’s shown)  \n5. **Group results by status** (Passed, Failed, Skipped) in the output table.  \n6. **Save** the full table in a **Markdown (.md)** file.\n\n\n## Output Table Format\n| Test Case Name | Status | Failed Keyword | Failure Reason (Plain Language) | Failure Reason (Detailed Summary) | Expected vs Actual | Root Cause Analysis |\n|----------------|--------|---------------|----------------------------------|------------------------------------|--------------------|---------------------|\n\n\n## Rules\n- Use **only visible data** from the HTML report — **no assumptions**.  \n- Do **not** generate or use Python code.  \n- Keep explanations **clear, concise, and accessible** for all readers, including non‑technical stakeholders.  \n- Maintain the **original wording** for technical details in the “Detailed Summary” column, but simplify the “Plain Language” column.  \n- Root cause analysis must be **evidence-based**.",
		"context": "Analysis Regression report",
		"version": "1.0"
	  },
	  {
		"serial": "2",
		"author": "Moulivenkat Naradasu",
		"functionality": "Test Case Execution Analysis prompt",
		"prompt": "## Task  \n\nYour responsibilities include:  \n- Executing Robot Framework test cases and monitoring execution until completion (via terminal output).  \n- Renaming generated reports into a strict unique format.\n- Analyze a given **Robot Framework HTML report** and produce a clear, human‑readable summary of all test results. \n- Performing expert-level report analysis, debugging, and troubleshooting using only the renamed report artifacts.  \n- Delivering a professional Markdown summary report suitable for stakeholders.  \n \n## OBJECTIVE\n1) Execute the given Robot Framework test run .  \n2) Confirm that the following reports are generated and renamed using a strict format:  \n   - REPORT_FILE = report_<RUN_LABEL>_<timestamp>.html  \n   (`<timestamp>` = YYYYMMDD_HHMMSS)  \n3) - **Open and review** the provided Robot Framework `REPORT_FILE` file.\n  -  **Identify** how the following are stored in the report:  \n   - Test case names  \n   - Keywords used in each test  \n   - Status of each test (Passed, Failed, Skipped)  \n   - Messages, logs, and error details  \n  -  **Extract** all test cases and list them with their status.  \n  - **For failed test cases**, also include:  \n   - **Failed keyword** (the keyword where the failure occurred)  \n   - **Failure reason/message** rewritten in **plain language** for non‑technical readers  \n   - **Detailed failure summary** (technical details from the report)  \n   - **Expected vs. actual values**  \n   - **Root cause analysis** at the *expert level*:  \n     - Explain what the test was expecting to happen  \n     - Explain what actually happened  \n     - Identify the most likely cause based only on visible evidence in the report (no assumptions beyond what’s shown)\n4) Perform expert-level analysis using the renamed files only.\n5) **Group results by status** (Passed, Failed, Skipped) in the output table.\n6) Always generate a professional Markdown report with findings.  \n \n## INPUTS  \n\n- TEST_CASE_FILE_PATH : \"C:\\Projects\\ABB\\Titan\\charger-test-framework-titan\\Tests\\Authorization\\Autocharge\\Pin.robot\" \n- TEST_CASE_TAG_ID: \"205412\" \n \n## CONSTRAINTS  \n\n- Use `robot --help` to check robot framework commands and confirm correct CLI options.\n- Do not assume causes outside the renamed reports. Only use evidence from REPORT_FILE.  \n- Always produce unique run directories or timestamped filenames.\n- If Test Case Failed, check failed keywords investigate deeply what caused to fail like a expert.\n- Report analysis, debugging, and troubleshooting are mandatory.\n\n## Rules\n- Use **only visible data** from the HTML report — **no assumptions**.  \n- Do **not** generate or use Python code.  \n- Keep explanations **clear, concise, and accessible** for all readers, including non‑technical stakeholders.  \n- Maintain the **original wording** for technical details in the “Detailed Summary” column, but simplify the “Plain Language” column.  \n- Root cause analysis must be **evidence-based**.\n\n## DELIVERABLES \n \n- Execution artifacts: REPORT_FILE (renamed uniquely).\n- **Root cause analysis** at the expert level\n- Supporting files: stdout, stderr, executed command.  \n- Markdown report (run_summary_<RUN_LABEL>_<timestamp>.md). \n \n## Output Table Format\n\n- Summary of test case\n- overview\n- Expected failed reason vs Actual failed reason, Why test case failed, what triggered to failed.\n- Expected = in which keyword test case failed that keyword what expecting results like(Charging, IDLE, OUT_OF_ORDER, Available....etc) & Actual = In that same failed keyword what we got results like (IDLE, Not_Started, Unavailable....etc). \n \n| Test Case Name | Status | Failed Keyword | Failure Reason (Plain Language) | Failure Reason (Detailed Summary) | Expected vs Actual | Root Cause Analysis |\n|----------------|--------|---------------|----------------------------------|------------------------------------|--------------------|---------------------|\n\n\n\n\n\n",
		"context": "Test Case Execution Analysis prompt",
		"version": "1.0"
	  },
	  {
		"serial": "3",
		"author": "Moulivenkat Naradasu",
		"functionality": "Sharing Error Logs Prompt",
		"prompt": "## Task:\n\n**Read and analyze** the logs in detail.  \n**Identify any issues, anomalies, or errors** present in the logs.  \n**Explain the root cause(s)** of the issues based on the log evidence.  \n**Highlight key events or patterns** that led to the issue.  \n**suggest possible next steps or fixes**.  \n**Root cause analysis** at the *expert level*\nReconstruct a concise Key Event Timeline with timestamps and short log excerpts for the path to failure/stop.\n**Think deeply and do research** before giving results.\n\n\n## Output format should be: \n\n**Summary of Issue**: Detailed, clear description of the problem found.  \n**Detailed Analysis**: Step-by-step reasoning from the log entries.  \n**Possible Causes**: Likely reasons for the issue.  \n**Recommendations**: Suggested actions to resolve or further investigate.  \nFocus only on what is observable from the provided file.  \nCreate markdown report including all details for this output fomat.\n\nPlease find the attachment\n\n### Following but not limited to thinking.",
		"context": "Sharing Error Logs Prompt",
		"version": "1.0"
	  },
	  {
		"serial": "4",
		"author": "Moulivenkat Naradasu",
		"functionality": "Regression Report Analysis & Bug Creation",
		"prompt": "##  Objective
Process test execution results from a Robot Framework HTML/XML report, compare them with historical data from the wiki, classify failures, and document findings in **two Markdown files**:

1. `bug_summary_<runID>.md` → Detailed summary with Section A tables.  
2. `bug_creation_<runID>.md` → Section B bug entries formatted for Azure DevOps bug creation.  

Then, using `bug_creation_<runID>.md`, **automatically create work items of type `Bug`** in the Titan Test Automation board.  
-> **Main Rule:** Bug creation is mandatory — every failed test case requiring a new bug must result in a bug created in the board.  
All bugs must strictly follow the **Defect Handling Guidelines**:  
-> https://abbevci.visualstudio.com/EVCI%20-%20Charger/_wiki/wikis/EVCI%20-%20Charger.wiki/51378/Defect-handling-in-Project-Titan  


## References
- **Board URL:** https://dev.azure.com/abbevci/EVCI%20-%20Charger/_boards/board/t/Titan%20Test%20Automation/Stories  
- **Defect Handling Guidelines:** https://abbevci.visualstudio.com/EVCI%20-%20Charger/_wiki/wikis/EVCI%20-%20Charger.wiki/51378/Defect-handling-in-Project-Titan  
- **Example Bug:** https://dev.azure.com/abbevci/EVCI%20-%20Charger/_boards/board/t/Titan%20Test%20Automation/Stories?System.WorkItemType=Bug&workitem=233741  


## Inputs
- **Report File (HTML/XML):**  
  `C:\Users\2380413\OneDrive - Cognizant\Documents\GenAI\Copilot_POC\Report_Analysis\output.xml`
- **Wiki Reference:**  
  `https://abbevci.visualstudio.com/EVCI%20-%20Charger/_wiki/wikis/EVCI%20-%20Charger.wiki/51399/Test-Automation-Marathon-ArmXL-2.0-1.7.94.29`


## Failure Classification Rules
1. **Bug Link Attached (Report/Azure):**  
   - Use the attached bug link directly.  
   - If the same test case is also failed in the wiki report, include it in this table with the linked bug ID and its current status.  
   - **If the linked bug is closed:**  
     - Create a new bug in the Titan Test Automation board.  
     - Document the new bug ID in Section A4 with remarks → **"Bug Created in Board: Titan Test Automation (Work Item ID)"**.  
     - Reference the closed bug ID in the new bug’s description for traceability.  

2. **Setup/Teardown Failures:**  
   - Remarks → `"Retest Needed"`  
   - Place in **Table A3**.  

3. **Simulator State Mismatch:**  
   - Remarks → `"Retest Needed"`  
   - Place in **Table A3**.  

4. **No Bug Link Attached:**  
   - Check wiki for historical failures and open bug references.  
   - If relevant bug found → attach bug link.  
   - If none found → **Create New Bug Entry** using Bug Content Requirements.  
   - Document in **Table A4** and **automatically create a work item of type Bug in Azure DevOps board**.  

5. **Mandatory Bug Creation Rule:**  
   - Every failed test case in Section A4 must have a valid bug link.  
   - If remark contains `#TBD` or no valid link, run the creation–verification loop until resolved.  
   - The run is complete only when Section A4 contains **zero TBDs**.

##️ Output: `bug_summary_<runID>.md`
Generate a Markdown file with the following sections:

### Section A1: Passed Test Cases
| SL No | Test Case ID | Test Case Name | Result |

### Section A2: Bug Link Attached
| SL No | Test Case ID | Test Case Name | Test Steps | Result | Failure Reason | Associated Bug Link | Remarks |

### Section A3: Retest Needed (Setup/Teardown + Simulator Mismatch)
| SL No | Test Case ID | Test Case Name | Test Steps | Result | Failure Reason | Associated Bug Link | Remarks → `"Retest Needed"` |

### Section A4: Other Failed Test Cases
| SL No | Test Case ID | Test Case Name | Test Steps | Result | Failure Reason | Associated Bug Link | Remarks |

### Section A5: Skipped Test Cases
| SL No | Test Case ID | Test Case Name | Result | Remarks → `"Skipped – Not Applicable"` or `"Skipped – Environment Issue"` |

## Bug Content Requirements
For each new bug candidate, include:
- **System.Title** → Concise summary with Test Case ID  
- **Microsoft.VSTS.TCM.SystemInfo** → Environment details (e.g., Marathon ArmXL-2, SW v1.7.94.29)  
- **Microsoft.VSTS.TCM.ReproSteps** → Detailed repro steps from test case evidence  
- **Custom.Stepstoreproduce** → Simplified repro steps  
- **Custom.WhichProductline** → Product cycle (e.g., Terra HP Gen 2 Gen 3)  
- **Custom.ExpectedBehavior** → From wiki/test case  
- **Custom.ObservedBehavior** → Failure reason/logs  
- **Custom.ImpactonUser** → Classification (Critical/Major/Minor)  
- **Custom.ImpactonUserdescription** → Plain-language impact description  
- **Microsoft.VSTS.Common.Severity** → Severity (Critical/Major/Minor)  
- **Microsoft.VSTS.Build.FoundIn** → Build version (e.g., 1.7.94.29)  
- **ScrumProcessImprovement.Groomed** → Default `0`  
- **Microsoft.VSTS.Common.ValueArea** → Default `Business`  


## Output Files

### File 1: `bug_summary_<runID>.md`
Contains Section A tables:
- Section A1: Passed Test Cases  
- Section A2: Bug Link Attached  
- Section A3: Retest Needed  
- Section A4: Other Failed Test Cases (remarks → **"Bug Created: [#<ID>](https://dev.azure.com/abbevci/EVCI%20-%20Charger/_workitems/edit/<ID>)"**)  

### File 2: `bug_creation_<runID>.md`
Contains Section B bug entries, formatted for Azure DevOps bug creation with all required fields.


## Automated Bug Creation

### REST API Example
```http
POST https://dev.azure.com/abbevci/EVCI%20-%20Charger/_apis/wit/workitems/$Bug?api-version=7.0
Content-Type: application/json-patch+json
Authorization: Basic <PAT>: "Please Paste here PAT Token"

[
  { "op": "add", "path": "/fields/System.Title", "value": "Bug for Test Case 208967" },
  { "op": "add", "path": "/fields/Microsoft.VSTS.TCM.SystemInfo", "value": "Marathon ArmXL-2, SW v1.7.94.29" },
  { "op": "add", "path": "/fields/Microsoft.VSTS.TCM.ReproSteps", "value": "Step1 → Step2 → Step3" },
  { "op": "add", "path": "/fields/Custom.Stepstoreproduce", "value": "Step1 → Step2 → Step3" },
  { "op": "add", "path": "/fields/Custom.WhichProductline", "value": "Terra HP Gen 2 Gen 3" },
  { "op": "add", "path": "/fields/Custom.ExpectedBehavior", "value": "Charging should start automatically" },
  { "op": "add", "path": "/fields/Custom.ObservedBehavior", "value": "Charger remained idle" },
  { "op": "add", "path": "/fields/Custom.ImpactonUser", "value": "Critical" },
  { "op": "add", "path": "/fields/Custom.ImpactonUserdescription", "value": "User cannot charge EV in production environment" },
  { "op": "add", "path": "/fields/Microsoft.VSTS.Common.Severity", "value": "Critical" },
  { "op": "add", "path": "/fields/Microsoft.VSTS.Build.FoundIn", "value": "1.7.94.29" },
  { "op": "add", "path": "/fields/ScrumProcessImprovement.Groomed", "value": "0" },
  { "op": "add", "path": "/fields/Microsoft.VSTS.Common.ValueArea", "value": "Business" }
]

## CLI
az boards work-item create \
  --type Bug \
  --title "Bug for Test Case 208967" \
  --description "Observed: Charger idle. Expected: Charging should start.\nSteps: Step1 → Step2 → Step3" \
  --area "Titan Test Automation team" \
  --iteration "EVCI - Charger\\Titan Test Automation - Test iteration" \
  --fields "Microsoft.VSTS.TCM.SystemInfo=Marathon ArmXL-2, SW v1.7.94.29" \
           "Custom.Stepstoreproduce=Step1 → Step2 → Step3" \
           "Custom.WhichProductline=Terra HP Gen 2 Gen 3" \
           "Custom.ExpectedBehavior=Charging should start automatically" \
           "Custom.ObservedBehavior=Charger remained idle" \
           "Custom.ImpactonUser=Critical" \
           "Custom.ImpactonUserdescription=User cannot charge EV in production environment" \
           "Microsoft.VSTS.Common.Severity=Critical" \
           "Microsoft.VSTS.Build.FoundIn=1.7.94.29" \
           "ScrumProcessImprovement.Groomed=0" \
           "Microsoft.VSTS.Common.ValueArea=Business"

## Important Instructions
Do not save automation scripts in workspace.
Execute REST API or CLI logic directly inside GitHub Actions, MCP server, or a terminal session.
Ensure portability — any team member can run automation without managing local files.
Create only one bug per test case.
Leave bugs unassigned.
Provide direct links to created bugs.

## Processing Flow
Parse report → extract test case data.
Classify failures → fill Section A tables.
Generate Section B bug entries in bug_creation_<runID>.md.
Call Azure DevOps REST API/CLI to create bugs with all required fields.
Capture returned Bug IDs and update Section A4 in bug_summary_<runID>.md.
Provide all bug links and numbers for verification.
If any bug is missing, create it and confirm again.

## Expected Outcome
Two markdown files generated (bug_summary_<runID>.md, bug_creation_<runID>.md).
All bugs created automatically in Titan Test Automation board with required fields.
Bug IDs and links confirmed, no pending

",
		"context": "Analysis & bug creation from given report",
		"version": "1.0"
	  }
		  
  ]
}
